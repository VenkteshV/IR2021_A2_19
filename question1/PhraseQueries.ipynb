{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PhraseQueries.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ikOL7PmhwIcG",
        "outputId": "9c919106-bc63-40e8-8d79-94b54fd98184"
      },
      "source": [
        "import joblib\n",
        "import functools\n",
        "import copy\n",
        "import operator\n",
        "import pickle\n",
        "import spacy\n",
        "import os\n",
        "import re, string, unicodedata\n",
        "import nltk\n",
        "!pip install contractions\n",
        "!pip install inflect\n",
        "import contractions\n",
        "import inflect\n",
        "from bs4 import BeautifulSoup\n",
        "from nltk import word_tokenize, sent_tokenize\n",
        "from nltk.corpus import stopwords,wordnet\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: contractions in /usr/local/lib/python3.7/dist-packages (0.0.48)\n",
            "Requirement already satisfied: textsearch>=0.0.21 in /usr/local/lib/python3.7/dist-packages (from contractions) (0.0.21)\n",
            "Requirement already satisfied: pyahocorasick in /usr/local/lib/python3.7/dist-packages (from textsearch>=0.0.21->contractions) (1.4.2)\n",
            "Requirement already satisfied: anyascii in /usr/local/lib/python3.7/dist-packages (from textsearch>=0.0.21->contractions) (0.1.7)\n",
            "Requirement already satisfied: inflect in /usr/local/lib/python3.7/dist-packages (2.1.0)\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B1NJk_R2FChT"
      },
      "source": [
        "def strip_html(text):\n",
        "    soup = BeautifulSoup(text, \"html.parser\")\n",
        "    return soup.get_text()\n",
        "\n",
        "def remove_between_square_brackets(text):\n",
        "    text=re.sub('\\n',' ',text)\n",
        "    return re.sub('\\[[^]]*\\]', '', text)\n",
        "\n",
        "def denoise_text(text):\n",
        "    text = strip_html(text)\n",
        "    text = remove_between_square_brackets(text)\n",
        "    return text\n",
        "\n",
        "def replace_contractions(text):\n",
        "    return contractions.fix(text)\n",
        "\n",
        "def remove_non_ascii(words):\n",
        "    \"\"\"Remove non-ASCII characters from list of tokenized words\"\"\"\n",
        "    new_words = []\n",
        "    for word in words:\n",
        "        new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
        "        new_words.append(new_word)\n",
        "    return new_words\n",
        "\n",
        "def to_lowercase(words):\n",
        "    \"\"\"Convert all characters to lowercase from list of tokenized words\"\"\"\n",
        "    new_words = []\n",
        "    for word in words:\n",
        "        new_word = word.lower()\n",
        "        new_words.append(new_word)\n",
        "    return new_words\n",
        "\n",
        "def remove_punctuation(words):\n",
        "    \"\"\"Remove punctuation from list of tokenized words\"\"\"\n",
        "    new_words = []\n",
        "    for word in words:\n",
        "        new_word = re.sub(r'[^\\w\\s]', '', word)\n",
        "        if new_word != '':\n",
        "            new_words.append(new_word)\n",
        "    return new_words\n",
        "\n",
        "def remove_stopwords(words):\n",
        "    \"\"\"Remove stop words from list of tokenized words\"\"\"\n",
        "    new_words = []\n",
        "    for word in words:\n",
        "        if word not in stopwords.words('english'):\n",
        "            new_words.append(word)\n",
        "    return new_words\n",
        "\n",
        "def get_wordnet_pos(tag):\n",
        "    tag_dict = {\"J\": wordnet.ADJ,\"N\": wordnet.NOUN,\"V\": wordnet.VERB,\"R\": wordnet.ADV}\n",
        "    return tag_dict.get(tag,wordnet.NOUN)\n",
        "\n",
        "# def lemmatize(words):\n",
        "#     \"\"\"Lemmatize words in list of tokenized words\"\"\"\n",
        "#     lemmatizer = WordNetLemmatizer()\n",
        "#     lemmas = []\n",
        "#     posTagged = nltk.pos_tag(words)\n",
        "#     wordnetTagged = list(map(lambda x: (x[0], get_wordnet_pos(x[1][0])), posTagged))\n",
        "#     for word,tag in wordnetTagged:\n",
        "#       lemma = lemmatizer.lemmatize(word,tag)\n",
        "#       lemmas.append(lemma)\n",
        "#     return lemmas\n",
        "\n",
        "def lemmatizeSpacy(words):\n",
        "  sent = \"\"\n",
        "  lwords = []\n",
        "  for word in words:\n",
        "    sent += word + \" \" \n",
        "  model = spacy.load(\"en_core_web_sm\")\n",
        "  tokens = model(sent)\n",
        "  # for i in range(len(tokens)):\n",
        "  #   lwords.append((tokens[i].lemma_,i+1))\n",
        "  for token in tokens:\n",
        "    lwords.append(token.lemma_)\n",
        "  return lwords\n",
        "\n",
        "def preProcess_html(fileName):\n",
        "    sample = denoise_text(fileName)\n",
        "    sample = replace_contractions(sample)\n",
        "    words = nltk.word_tokenize(sample)\n",
        "    words = remove_non_ascii(words)\n",
        "    words = to_lowercase(words)\n",
        "    words = remove_punctuation(words)\n",
        "    words = lemmatizeSpacy(words)\n",
        "    words = remove_stopwords(words)\n",
        "    # words = lemmatize(words)\n",
        "    # words = lemmatizeSpacy(words)\n",
        "    return words  \n",
        "\n",
        "def clean_text(text):\n",
        "    # text=re.sub('\\w*\\d\\w*','', text)\n",
        "    text=re.sub('\\n',' ',text)\n",
        "    text=re.sub(r\"http\\S+\", \"\", text)\n",
        "    text=re.sub('[^a-z0-9A-Z]',' ',text)\n",
        "    text=re.sub(' +',' ',text)\n",
        "    return text\n",
        "\n",
        "def preProcessSentence(fileName):\n",
        "    sample = clean_text(fileName)\n",
        "    sample = replace_contractions(sample)\n",
        "    words = nltk.word_tokenize(sample)\n",
        "    words = remove_non_ascii(words)\n",
        "    words = to_lowercase(words)\n",
        "    words = remove_punctuation(words)\n",
        "    words = remove_stopwords(words)\n",
        "    words = lemmatizeSpacy(words)    \n",
        "    return words"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R2fX6B6Vyjbi"
      },
      "source": [
        "def getDocNames(dt,ListdocId):\n",
        "  Docnames = []\n",
        "  for Sdocid in ListdocId:\n",
        "    for docname, docid in dt.items():  \n",
        "      if Sdocid == docid:\n",
        "        Docnames.append(docname)\n",
        "  return Docnames\n",
        "\n",
        "def oneWord(Q,positional_index,document_index_map):\n",
        "  if Q not in positional_index.keys():\n",
        "    print(\"Terms Not Present\")\n",
        "    print(\"Number of documents retrieved: {0}\".format(0))\n",
        "    print(\"List of document names retrieved: --\")\n",
        "  else:\n",
        "    p = positional_index[Q][1].keys()\n",
        "    Listdocs = getDocNames(document_index_map,p)\n",
        "    print(\"Number of documents retrieved: {0}\".format(len(Listdocs)))\n",
        "    print(\"List of document names retrieved: {0}\".format(\", \".join(Listdocs)))\n",
        "\n",
        "def getPostings(terms,dt):\n",
        "    #all terms in the list are guaranteed to be in the index\n",
        "    return [ dt[term] for term in terms ]\n",
        "\n",
        "\n",
        "def getDocsFromPostings(postings):\n",
        "    #no empty list in postings\n",
        "    return [ [x[0] for x in p] for p in postings ]\n",
        "\n",
        "def intersectLists(lists):\n",
        "    if len(lists)==0:\n",
        "        return []\n",
        "    lists.sort(key=len)\n",
        "    return list(functools.reduce(lambda x,y: set(x)&set(y),lists))\n",
        "\n",
        "def getDictionary(query,positional_index):\n",
        "  d = {}\n",
        "  for i in query:\n",
        "    d[i] = [[x,positional_index[i][1][x]] for x in positional_index[i][1]]\n",
        "  return d\n",
        "\n",
        "def phraseQueries(Query,positional_index,document_index_map):\n",
        "  for term in Query:\n",
        "    if term not in positional_index.keys():\n",
        "      print(\"Terms Not Found\")\n",
        "      print(\"Number of documents retrieved: {0}\".format(0))\n",
        "      print(\"List of document names retrieved: --\")\n",
        "      return\n",
        "  dictionary = getDictionary(Query,positional_index)\n",
        "  postings = getPostings(Query,dictionary)\n",
        "  # print(postings)\n",
        "  docs = getDocsFromPostings(postings)\n",
        "  # print(docs)\n",
        "  docs = intersectLists(docs) \n",
        "  # print(docs)\n",
        "  for i in range(len(postings)):\n",
        "    postings[i]=[x for x in postings[i] if x[0] in docs]\n",
        "  # print(postings)\n",
        "  postings=copy.deepcopy(postings)\n",
        "  for i in range(len(postings)):\n",
        "    for j in range(len(postings[i])):\n",
        "      postings[i][j][1]=[x-i for x in postings[i][j][1]]\n",
        "\n",
        "  # print(postings)\n",
        "  result=[]\n",
        "  for i in range(len(postings[0])):\n",
        "    li=intersectLists([x[i][1] for x in postings])\n",
        "    if li==[]:\n",
        "      continue\n",
        "    else:\n",
        "      result.append(postings[0][i][0])    #append the docid to the result\n",
        "  if len(result) == 0:\n",
        "    print(\"Number of documents retrieved: {0}\".format(0))\n",
        "    print(\"List of document names retrieved: --\")\n",
        "  else:\n",
        "    print(\"Number of documents retrieved: {0}\".format(len(result)))\n",
        "    print(\"List of document names retrieved: {0}\".format(\", \".join(getDocNames(document_index_map,result))))"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x-NxxNOfQXxg",
        "outputId": "edee0406-eb59-4cc3-8db0-9c426a674b81"
      },
      "source": [
        "if __name__== \"__main__\":\n",
        "  positional_index = joblib.load('/content/positional_indexes')\n",
        "  document_index_map = joblib.load('/content/document_index_map')\n",
        "  document_index_map_keys = list(document_index_map.keys())\n",
        "  document_index_map_values = list(document_index_map.values())\n",
        "  number_of_queries = int(input(\"Enter number of queries:   \"))\n",
        "  # Queries = [\"plum-coloured swelling\",\"Telephones jangle and typewriters\",\"good government\",\"the mountains bordering\",\"good and day\",\"good day\",\"rabbits, hares, partridges and skylarks\",\"The terrified citizens flocked\",\"Sherlock Holmes\",\"the lion, the tiger\",\"lion tiger\",\"I Cried\",\"British bark Sophy Anderson\",\"Gfile Distribution Center\",\"Abbey Grange, Marsham\",\"Descent and Easy\",\"AND ON THAT BRANCH THERE WAS A NEST\"]\n",
        "  for q in range(number_of_queries):\n",
        "    query = str(input(\"Enter the query with terms separated by space:  \"))\n",
        "    print(\"Query : \"+query)\n",
        "    if(not (query and not query.isspace())):\n",
        "      print(\"Empty Query\")\n",
        "    else:\n",
        "      query = preProcessSentence(query)\n",
        "      print(\"Query Terms: {0}\".format(query))\n",
        "      if len(query) == 1:\n",
        "        oneWord(query[0],positional_index,document_index_map)\n",
        "      else:\n",
        "        phraseQueries(query,positional_index,document_index_map)\n",
        "    \n",
        "    print(\"\\n\")"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Enter number of queries:   1\n",
            "Enter the query with terms separated by space:  The terrified citizens flocked.\n",
            "Query : The terrified citizens flocked.\n",
            "Query Terms: ['terrified', 'citizen', 'flock']\n",
            "Number of documents retrieved: 1\n",
            "List of document names retrieved: pphamlin.txt\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qbt7ItN3QXtQ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6YYWu3OHDIyV"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}