{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PhraseQueries.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "uWgk1l7sVA77"
      },
      "source": [
        "import joblib\n",
        "import functools\n",
        "import copy\n",
        "import operator"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ikOL7PmhwIcG",
        "outputId": "094af393-61b4-443c-f4a9-f6a234e8e3a5"
      },
      "source": [
        "import pickle\n",
        "import os\n",
        "import re, string, unicodedata\n",
        "import nltk\n",
        "!pip install contractions\n",
        "!pip install inflect\n",
        "import spacy\n",
        "import contractions\n",
        "import inflect\n",
        "from bs4 import BeautifulSoup\n",
        "from nltk import word_tokenize, sent_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords') \n",
        "\n",
        "def strip_html(text):\n",
        "    soup = BeautifulSoup(text, \"html.parser\")\n",
        "    return soup.get_text()\n",
        "\n",
        "def remove_between_square_brackets(text):\n",
        "    text=re.sub('\\n',' ',text)\n",
        "    return re.sub('\\[[^]]*\\]', '', text)\n",
        "\n",
        "def denoise_text(text):\n",
        "    text = strip_html(text)\n",
        "    text = remove_between_square_brackets(text)\n",
        "    return text\n",
        "\n",
        "def replace_contractions(text):\n",
        "    return contractions.fix(text)\n",
        "\n",
        "def remove_non_ascii(words):\n",
        "    \"\"\"Remove non-ASCII characters from list of tokenized words\"\"\"\n",
        "    new_words = []\n",
        "    for word in words:\n",
        "        new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
        "        new_words.append(new_word)\n",
        "    return new_words\n",
        "\n",
        "def to_lowercase(words):\n",
        "    \"\"\"Convert all characters to lowercase from list of tokenized words\"\"\"\n",
        "    new_words = []\n",
        "    for word in words:\n",
        "        new_word = word.lower()\n",
        "        new_words.append(new_word)\n",
        "    return new_words\n",
        "\n",
        "def remove_punctuation(words):\n",
        "    \"\"\"Remove punctuation from list of tokenized words\"\"\"\n",
        "    new_words = []\n",
        "    for word in words:\n",
        "        new_word = re.sub(r'[^\\w\\s]', '', word)\n",
        "        if new_word != '':\n",
        "            new_words.append(new_word)\n",
        "    return new_words\n",
        "\n",
        "def remove_stopwords(words):\n",
        "    \"\"\"Remove stop words from list of tokenized words\"\"\"\n",
        "    new_words = []\n",
        "    for word in words:\n",
        "        if word not in stopwords.words('english'):\n",
        "            new_words.append(word)\n",
        "    return new_words\n",
        "\n",
        "def get_wordnet_pos(tag):\n",
        "    tag_dict = {\"J\": wordnet.ADJ,\"N\": wordnet.NOUN,\"V\": wordnet.VERB,\"R\": wordnet.ADV}\n",
        "    return tag_dict.get(tag,wordnet.NOUN)\n",
        "\n",
        "def lemmatize(words):\n",
        "    \"\"\"Lemmatize words in list of tokenized words\"\"\"\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    lemmas = []\n",
        "    posTagged = nltk.pos_tag(words)\n",
        "    wordnetTagged = list(map(lambda x: (x[0], get_wordnet_pos(x[1][0])), posTagged))\n",
        "    for word,tag in wordnetTagged:\n",
        "      lemma = lemmatizer.lemmatize(word,tag)\n",
        "      lemmas.append(lemma)\n",
        "    return lemmas\n",
        "def lemmatizeSpacy(words):\n",
        "    sent = \"\"\n",
        "    lwords = []\n",
        "    for word in words:\n",
        "        sent += word + \" \" \n",
        "    model = spacy.load(\"en_core_web_sm\")\n",
        "    tokens = model(sent)\n",
        "    for token in tokens:\n",
        "        lwords.append(token.lemma_)\n",
        "    return lwords\n",
        "def preProcess_html(fileName):\n",
        "    sample = denoise_text(fileName)\n",
        "    sample = replace_contractions(sample)\n",
        "    words = nltk.word_tokenize(sample)\n",
        "    words = remove_non_ascii(words)\n",
        "    words = to_lowercase(words)\n",
        "    words = remove_punctuation(words)\n",
        "    words = remove_stopwords(words)\n",
        "    words = lemmatize(words)\n",
        "    return words  \n",
        "\n",
        "def clean_text(text):\n",
        "    # text=re.sub('\\w*\\d\\w*','', text)\n",
        "    text=re.sub('\\n',' ',text)\n",
        "    text=re.sub(r\"http\\S+\", \"\", text)\n",
        "    text=re.sub('[^a-z0-9A-Z]',' ',text)\n",
        "    text=re.sub(' +',' ',text)\n",
        "    return text\n",
        "\n",
        "def preProcessSentence(sentence):\n",
        "    sample = clean_text(sentence)\n",
        "    sample = replace_contractions(sample)\n",
        "    words = nltk.word_tokenize(sample)\n",
        "    words = remove_non_ascii(words)\n",
        "    words = to_lowercase(words)\n",
        "    words = remove_punctuation(words)\n",
        "    words = remove_stopwords(words)\n",
        "    words = lemmatizeSpacy(words)\n",
        "    return words "
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: contractions in /usr/local/lib/python3.7/dist-packages (0.0.48)\n",
            "Requirement already satisfied: textsearch>=0.0.21 in /usr/local/lib/python3.7/dist-packages (from contractions) (0.0.21)\n",
            "Requirement already satisfied: pyahocorasick in /usr/local/lib/python3.7/dist-packages (from textsearch>=0.0.21->contractions) (1.4.2)\n",
            "Requirement already satisfied: anyascii in /usr/local/lib/python3.7/dist-packages (from textsearch>=0.0.21->contractions) (0.1.7)\n",
            "Requirement already satisfied: inflect in /usr/local/lib/python3.7/dist-packages (2.1.0)\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kxi5gG000sXT"
      },
      "source": [
        "def getDocNames(dt,ListdocId):\n",
        "  Docnames = []\n",
        "  for Sdocid in ListdocId:\n",
        "    for docname, docid in dt.items():  \n",
        "      if Sdocid == docid:\n",
        "        Docnames.append(docname)\n",
        "  return Docnames"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R2fX6B6Vyjbi"
      },
      "source": [
        "def oneWord(Q,positional_index,document_index_map):\n",
        "  if Q not in positional_index.keys():\n",
        "    print(\"Terms Not Present\")\n",
        "    print(\"Number of documents retrieved: {0}\".format(0))\n",
        "    print(\"List of document names retrieved: --\")\n",
        "  else:\n",
        "    p = positional_index[Q][1].keys()\n",
        "    Listdocs = getDocNames(document_index_map,p)\n",
        "    print(\"Number of documents retrieved: {0}\".format(len(Listdocs)))\n",
        "    print(\"List of document names retrieved: {0}\".format(\", \".join(Listdocs)))"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kU9F2h1X7Jwt"
      },
      "source": [
        "def getPostings(terms,dt):\n",
        "    #all terms in the list are guaranteed to be in the index\n",
        "    return [ dt[term] for term in terms ]\n",
        "\n",
        "\n",
        "def getDocsFromPostings(postings):\n",
        "    #no empty list in postings\n",
        "    return [ [x[0] for x in p] for p in postings ]\n",
        "\n",
        "def intersectLists(lists):\n",
        "    if len(lists)==0:\n",
        "        return []\n",
        "    lists.sort(key=len)\n",
        "    return list(functools.reduce(lambda x,y: set(x)&set(y),lists))"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uwbSznDRyAMX"
      },
      "source": [
        "def getDictionary(query,positional_index):\n",
        "  d = {}\n",
        "  for i in query:\n",
        "    d[i] = [[x,positional_index[i][1][x]] for x in positional_index[i][1]]\n",
        "  return d"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qKi2DrvV4eS-"
      },
      "source": [
        "def phraseQueries(Query,positional_index,document_index_map):\n",
        "  for term in Query:\n",
        "    if term not in positional_index.keys():\n",
        "      print(\"Terms Not Found\")\n",
        "      print(\"Number of documents retrieved: {0}\".format(0))\n",
        "      print(\"List of document names retrieved: --\")\n",
        "      return\n",
        "  dictionary = getDictionary(Query,positional_index)\n",
        "  postings = getPostings(Query,dictionary)\n",
        "  # print(postings)\n",
        "  docs = getDocsFromPostings(postings)\n",
        "  # print(docs)\n",
        "  docs = intersectLists(docs) \n",
        "  # print(docs)\n",
        "  for i in range(len(postings)):\n",
        "    postings[i]=[x for x in postings[i] if x[0] in docs]\n",
        "  # print(postings)\n",
        "  postings=copy.deepcopy(postings)\n",
        "  for i in range(len(postings)):\n",
        "    for j in range(len(postings[i])):\n",
        "      postings[i][j][1]=[x-i for x in postings[i][j][1]]\n",
        "\n",
        "  # print(postings)\n",
        "  result=[]\n",
        "  for i in range(len(postings[0])):\n",
        "    li=intersectLists([x[i][1] for x in postings])\n",
        "    if li==[]:\n",
        "      continue\n",
        "    else:\n",
        "      result.append(postings[0][i][0])    #append the docid to the result\n",
        "  if len(result) == 0:\n",
        "    print(\"Number of documents retrieved: {0}\".format(0))\n",
        "    print(\"List of document names retrieved: --\")\n",
        "  else:\n",
        "    print(\"Number of documents retrieved: {0}\".format(len(result)))\n",
        "    print(\"List of document names retrieved: {0}\".format(\", \".join(getDocNames(document_index_map,result))))"
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x-NxxNOfQXxg",
        "outputId": "af80f342-5a43-432c-f6d7-90860af5fb62"
      },
      "source": [
        "if __name__==\"__main__\":\n",
        "  positional_index = joblib.load('/content/positional_indexes')\n",
        "  document_index_map = joblib.load('/content/document_index_map')\n",
        "  document_index_map_keys = list(document_index_map.keys())\n",
        "  document_index_map_values = list(document_index_map.values())\n",
        "  #number_of_queries = int(input(\"Enter number of queries\"))\n",
        "  number_of_queries = 6\n",
        "  Queries = [\"I Cried\",\"British bark Sophy Anderson\",\"Gfile Distribution Center\",\"Abbey Grange, Marsham\",\"Descent and Easy\",\"AND ON THAT BRANCH THERE WAS A NEST\"]\n",
        "  for q in range(number_of_queries):\n",
        "    #query = str(input(\"Enter the query with terms separated by space\"))\n",
        "    query = Queries[q]\n",
        "    print(\"Query : \"+query)\n",
        "    if(not (query and not query.isspace())):\n",
        "      print(\"Empty Query\")\n",
        "    else:\n",
        "      query = preProcessSentence(query)\n",
        "      if len(query) == 1:\n",
        "        oneWord(query[0],positional_index,document_index_map)\n",
        "      else:\n",
        "        phraseQueries(query,positional_index,document_index_map)\n",
        "    \n",
        "    print(\"\\n\\n\\n\")"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Query : I Cried\n",
            "Number of documents retrieved: 178\n",
            "List of document names retrieved: timem.hac, sight.txt, beyond.hum, nitepeek.sto, advsayed.txt, eyeargon.hum, elite.app, rocket.sf, corcor.hum, ab40thv.txt, gold3ber.txt, game.txt, knuckle.txt, empnclot.txt, abbey.txt, advtthum.txt, archive, wlgirl.txt, wolf7kid.txt, vday.hum, wombat.und, enchdup.hum, tcoa.txt, enginer.txt, encamp01.txt, korea.s, taxnovel.txt, keepmodu.txt, vainsong.txt, goldbug.poe, grav, empty.txt, gulliver.txt, gay, ghost, tao3.dos, enya_trn.txt, ezoff, goldenp.txt, yukon.txt, veiledl.txt, alissadl.txt, aquith.txt, bulzork1.txt, bulironb.txt, bureau.txt, burintrv.92, mindprob.txt, zombies.txt, wisteria.txt, outcast.dos, quest, beggars.txt, bishop00.txt, blackp.txt, blh.txt, buldream.txt, bulmrx.txt, bulnland.txt, arcadia.sty, tuc_mees, mattress.txt, mazarin.txt, tearglas.txt, 13chil.txt, 18.lws, 19.lws, 5orange.txt, 6napolen.txt, 7voysinb.txt, musgrave.txt, jackbstl.txt, 3gables.txt, 3student.txt, radar_ra.txt, shoscomb.txt, sick-kid.txt, silverb.txt, sleprncs.txt, snowqn1.txt, piracy.sto, hansgrtl.txt, hareleph.txt, blabnove.hum, blabnove.txt, hitch2.txt, hitch3.txt, idi.hum, imonly17.txt, darkness.txt, bran, breaks1.asc, breaks2.asc, breaks3.asc, bruce-p.txt, lionmane.txt, lmermaid.txt, lmtchgrl.txt, startrek.txt, diaryflf.txt, dicksong.txt, long1-3.txt, lpeargrl.txt, lrrhood.txt, ltp, floc, flute.txt, parotsha.txt, perf, missing.txt, pussboot.txt, pinocch.txt, dopedenn.txt, dskool.txt, redragon.txt, rock, running.txt, superg1, igiv, adler.txt, aesop11.txt, aesopa10.txt, alad10.txt, fable.txt, spectacl.poe, sqzply.txt, sre-dark.txt, solitary.txt, pregn.txt, psf.txt, psi, psyc, cybersla.txt, hop-frog.poe, hound-b.txt, fic1, fic7, fran, fgoose.txt, freeman.fil, nigel.10, nigel.3, nigel.5, nigel.6, nigel.7, nihgel_8.9, nigel.1, 4moons.txt, telefone.txt, history5.txt, pphamlin.txt, chik, clon, jackmac.fic, fourth.fic, pepdegener.txt, socialvikings.txt, fowl.death, pepsi.degenerat, social.vikings, robotech, hellmach.txt, bookem2, bookem3, 100west.txt, quarter.c1, quarter.c11, quarter.c15, quarter.c18, quarter.c2, quarter.c3, quarter.c5, quarter.c7, vgilante.txt, sre03.txt, sre_feqh.txt, sre10.txt, sre09.txt\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Query : British bark Sophy Anderson\n",
            "Number of documents retrieved: 2\n",
            "List of document names retrieved: 5orange.txt, holmesbk.txt\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Query : Gfile Distribution Center\n",
            "Number of documents retrieved: 1\n",
            "List of document names retrieved: imonly17.txt\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Query : Abbey Grange, Marsham\n",
            "Number of documents retrieved: 1\n",
            "List of document names retrieved: abbey.txt\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Query : Descent and Easy\n",
            "Number of documents retrieved: 2\n",
            "List of document names retrieved: imonly17.txt, fic3\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Query : AND ON THAT BRANCH THERE WAS A NEST\n",
            "Number of documents retrieved: 1\n",
            "List of document names retrieved: the-tree.txt\n",
            "\n",
            "\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qbt7ItN3QXtQ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4d_4WlJXT9GX"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}