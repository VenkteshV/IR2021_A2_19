{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "preprocessing.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "HEGolzhYDpMY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b64d2925-a3e1-4f02-ddb1-f0ec0286d406"
      },
      "source": [
        "import pickle\n",
        "import spacy\n",
        "import os\n",
        "import re, string, unicodedata\n",
        "import nltk\n",
        "!pip install contractions\n",
        "!pip install inflect\n",
        "import contractions\n",
        "import inflect\n",
        "from bs4 import BeautifulSoup\n",
        "from nltk import word_tokenize, sent_tokenize\n",
        "from nltk.corpus import stopwords,wordnet\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: contractions in /usr/local/lib/python3.7/dist-packages (0.0.48)\n",
            "Requirement already satisfied: textsearch>=0.0.21 in /usr/local/lib/python3.7/dist-packages (from contractions) (0.0.21)\n",
            "Requirement already satisfied: pyahocorasick in /usr/local/lib/python3.7/dist-packages (from textsearch>=0.0.21->contractions) (1.4.2)\n",
            "Requirement already satisfied: anyascii in /usr/local/lib/python3.7/dist-packages (from textsearch>=0.0.21->contractions) (0.1.7)\n",
            "Requirement already satisfied: inflect in /usr/local/lib/python3.7/dist-packages (2.1.0)\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ecgzA6gwExdE"
      },
      "source": [
        "def DoctoDict():\n",
        "  file_Names = os.listdir(\"/content/drive/MyDrive/IR_ASSIGNMENT_1/stories1\")\n",
        "  file_Paths = []\n",
        "  for i in range(len(file_Names)):\n",
        "    file_Paths.append(\"/content/drive/MyDrive/IR_ASSIGNMENT_1/stories1/\"+file_Names[i])\n",
        "  corpus = {}\n",
        "  for i in range(len(file_Paths)):\n",
        "    with open(file_Paths[i],encoding = \"latin-1\") as f_input:\n",
        "      corpus[file_Names[i]] = [f_input.read()]\n",
        "  return corpus"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DVYdVrsbI7JJ"
      },
      "source": [
        "CORPUS = DoctoDict()\n",
        "with open('CORPUS.pickle', 'wb') as handle:\n",
        "  pickle.dump(CORPUS, handle, protocol=pickle.HIGHEST_PROTOCOL)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ICteejq_wa7a",
        "outputId": "af42bc59-1024-49b0-d6b6-c0e6b77dbf5c"
      },
      "source": [
        "with open('/content/CORPUS.pickle', 'rb') as handle:\n",
        "  corpus = pickle.load(handle)\n",
        "\n",
        "len(corpus.keys())"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "467"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eM6mJ8drTPNA"
      },
      "source": [
        "def strip_html(text):\n",
        "    soup = BeautifulSoup(text, \"html.parser\")\n",
        "    return soup.get_text()\n",
        "\n",
        "def remove_between_square_brackets(text):\n",
        "    text=re.sub('\\n',' ',text)\n",
        "    return re.sub('\\[[^]]*\\]', '', text)\n",
        "\n",
        "def denoise_text(text):\n",
        "    text = strip_html(text)\n",
        "    text = remove_between_square_brackets(text)\n",
        "    return text\n",
        "\n",
        "def replace_contractions(text):\n",
        "    return contractions.fix(text)\n",
        "\n",
        "def remove_non_ascii(words):\n",
        "    \"\"\"Remove non-ASCII characters from list of tokenized words\"\"\"\n",
        "    new_words = []\n",
        "    for word in words:\n",
        "        new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
        "        new_words.append(new_word)\n",
        "    return new_words\n",
        "\n",
        "def to_lowercase(words):\n",
        "    \"\"\"Convert all characters to lowercase from list of tokenized words\"\"\"\n",
        "    new_words = []\n",
        "    for word in words:\n",
        "        new_word = word.lower()\n",
        "        new_words.append(new_word)\n",
        "    return new_words\n",
        "\n",
        "def remove_punctuation(words):\n",
        "    \"\"\"Remove punctuation from list of tokenized words\"\"\"\n",
        "    new_words = []\n",
        "    for word in words:\n",
        "        new_word = re.sub(r'[^\\w\\s]', '', word)\n",
        "        if new_word != '':\n",
        "            new_words.append(new_word)\n",
        "    return new_words\n",
        "\n",
        "def remove_stopwords(words):\n",
        "    \"\"\"Remove stop words from list of tokenized words\"\"\"\n",
        "    new_words = []\n",
        "    for word in words:\n",
        "        if word not in stopwords.words('english'):\n",
        "            new_words.append(word)\n",
        "    return new_words\n",
        "\n",
        "def get_wordnet_pos(tag):\n",
        "    tag_dict = {\"J\": wordnet.ADJ,\"N\": wordnet.NOUN,\"V\": wordnet.VERB,\"R\": wordnet.ADV}\n",
        "    return tag_dict.get(tag,wordnet.NOUN)\n",
        "\n",
        "# def lemmatize(words):\n",
        "#     \"\"\"Lemmatize words in list of tokenized words\"\"\"\n",
        "#     lemmatizer = WordNetLemmatizer()\n",
        "#     lemmas = []\n",
        "#     posTagged = nltk.pos_tag(words)\n",
        "#     wordnetTagged = list(map(lambda x: (x[0], get_wordnet_pos(x[1][0])), posTagged))\n",
        "#     for word,tag in wordnetTagged:\n",
        "#       lemma = lemmatizer.lemmatize(word,tag)\n",
        "#       lemmas.append(lemma)\n",
        "#     return lemmas\n",
        "\n",
        "def lemmatizeSpacy(words):\n",
        "  sent = \"\"\n",
        "  lwords = []\n",
        "  for word in words:\n",
        "    sent += word + \" \" \n",
        "  model = spacy.load(\"en_core_web_sm\")\n",
        "  tokens = model(sent)\n",
        "  for i in range(len(tokens)):\n",
        "    lwords.append((tokens[i].lemma_,i+1))\n",
        "  # for token in tokens:\n",
        "  #   lwords.append(token.lemma_)\n",
        "  return lwords\n",
        "\n",
        "def preProcess_html(fileName):\n",
        "    sample = denoise_text(fileName)\n",
        "    sample = replace_contractions(sample)\n",
        "    words = nltk.word_tokenize(sample)\n",
        "    words = remove_non_ascii(words)\n",
        "    words = to_lowercase(words)\n",
        "    words = remove_punctuation(words)\n",
        "    words = lemmatizeSpacy(words)\n",
        "    words = remove_stopwords(words)\n",
        "    # words = lemmatize(words)\n",
        "    # words = lemmatizeSpacy(words)\n",
        "    return words  \n",
        "\n",
        "def clean_text(text):\n",
        "    # text=re.sub('\\w*\\d\\w*','', text)\n",
        "    text=re.sub('\\n',' ',text)\n",
        "    text=re.sub(r\"http\\S+\", \"\", text)\n",
        "    text=re.sub('[^a-z0-9A-Z]',' ',text)\n",
        "    text=re.sub(' +',' ',text)\n",
        "    return text\n",
        "\n",
        "def preProcessotherfiles(fileName):\n",
        "    sample = clean_text(fileName)\n",
        "    sample = replace_contractions(sample)\n",
        "    words = nltk.word_tokenize(sample)\n",
        "    words = remove_non_ascii(words)\n",
        "    words = to_lowercase(words)\n",
        "    words = remove_punctuation(words)\n",
        "    # words = remove_stopwords(words)\n",
        "    words = lemmatizeSpacy(words)    \n",
        "    return words"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OTwRbkWFuwTS"
      },
      "source": [
        ""
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eFsKQI7MlBg7"
      },
      "source": [
        "for i in corpus.keys():\n",
        "  if i.endswith(\".html\") or i.endswith(\".header\"):\n",
        "    corpus[i][0] = preProcess_html(corpus[i][0])\n",
        "  else:\n",
        "    corpus[i][0] = preProcessotherfiles(corpus[i][0])"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8MS58BBPjf-d",
        "outputId": "faac74d2-d758-4859-ba89-24ae03e64de6"
      },
      "source": [
        "len(corpus.keys())"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "467"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WOsI8P7azn0X",
        "outputId": "d4623652-630e-45b5-9785-8b08ec62a6ce"
      },
      "source": [
        "print(corpus['redragon.txt'])"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[('the', 1), ('red', 2), ('dragon', 3), ('once', 4), ('there', 5), ('be', 6), ('a', 7), ('time', 8), ('thousand', 9), ('of', 10), ('year', 11), ('ago', 12), ('when', 13), ('animal', 14), ('be', 15), ('not', 16), ('the', 17), ('same', 18), ('as', 19), ('-PRON-', 20), ('be', 21), ('now', 22), ('except', 23), ('for', 24), ('a', 25), ('few', 26), ('like', 27), ('the', 28), ('lion', 29), ('the', 30), ('tiger', 31), ('and', 32), ('the', 33), ('butterfly', 34), ('-PRON-', 35), ('all', 36), ('look', 37), ('alike', 38), ('all', 39), ('be', 40), ('more', 41), ('or', 42), ('less', 43), ('the', 44), ('same', 45), ('height', 46), ('everyone', 47), ('have', 48), ('four', 49), ('leg', 50), ('and', 51), ('-PRON-', 52), ('wasn', 53), ('t', 54), ('easy', 55), ('to', 56), ('tell', 57), ('which', 58), ('be', 59), ('which', 60), ('even', 61), ('though', 62), ('the', 63), ('elephant', 64), ('do', 65), ('weigh', 66), ('more', 67), ('than', 68), ('the', 69), ('hyena', 70), ('and', 71), ('the', 72), ('hippo', 73), ('more', 74), ('than', 75), ('the', 76), ('gazelle', 77), ('one', 78), ('day', 79), ('while', 80), ('all', 81), ('the', 82), ('animal', 83), ('be', 84), ('relax', 85), ('in', 86), ('a', 87), ('field', 88), ('along', 89), ('come', 90), ('a', 91), ('red', 92), ('dragon', 93), ('out', 94), ('of', 95), ('breath', 96), ('cry', 97), ('-PRON-', 98), ('re', 99), ('in', 100), ('danger', 101), ('folk', 102), ('the', 103), ('world', 104), ('be', 105), ('about', 106), ('to', 107), ('come', 108), ('to', 109), ('an', 110), ('end', 111), ('how', 112), ('do', 113), ('-PRON-', 114), ('know', 115), ('everyone', 116), ('ask', 117), ('the', 118), ('dragon', 119), ('reply', 120), ('i', 121), ('read', 122), ('-PRON-', 123), ('in', 124), ('the', 125), ('star', 126), ('-PRON-', 127), ('must', 128), ('escape', 129), ('but', 130), ('where', 131), ('can', 132), ('-PRON-', 133), ('go', 134), ('-PRON-', 135), ('ask', 136), ('-PRON-', 137), ('to', 138), ('another', 139), ('world', 140), ('-PRON-', 141), ('reply', 142), ('i', 143), ('will', 144), ('take', 145), ('-PRON-', 146), ('there', 147), ('i', 148), ('can', 149), ('fly', 150), ('and', 151), ('i', 152), ('will', 153), ('take', 154), ('-PRON-', 155), ('to', 156), ('a', 157), ('planet', 158), ('that', 159), ('be', 160), ('safe', 161), ('than', 162), ('this', 163), ('one', 164), ('frighten', 165), ('as', 166), ('-PRON-', 167), ('be', 168), ('all', 169), ('the', 170), ('animal', 171), ('climb', 172), ('on', 173), ('to', 174), ('the', 175), ('dragon', 176), ('s', 177), ('back', 178), ('with', 179), ('a', 180), ('bored', 181), ('look', 182), ('the', 183), ('lion', 184), ('say', 185), ('i', 186), ('m', 187), ('not', 188), ('scared', 189), ('of', 190), ('anything', 191), ('so', 192), ('i', 193), ('will', 194), ('just', 195), ('stay', 196), ('here', 197), ('on', 198), ('earth', 199), ('the', 200), ('other', 201), ('however', 202), ('be', 203), ('fight', 204), ('to', 205), ('get', 206), ('on', 207), ('the', 208), ('dragon', 209), ('s', 210), ('back', 211), ('don', 212), ('t', 213), ('push', 214), ('-PRON-', 215), ('behind', 216), ('shout', 217), ('the', 218), ('crocodile', 219), ('hey', 220), ('move', 221), ('that', 222), ('paw', 223), ('-PRON-', 224), ('be', 225), ('just', 226), ('like', 227), ('people', 228), ('today', 229), ('push', 230), ('and', 231), ('shove', 232), ('to', 233), ('get', 234), ('onto', 235), ('an', 236), ('overcrowded', 237), ('train', 238), ('at', 239), ('last', 240), ('the', 241), ('dragon', 242), ('cry', 243), ('ready', 244), ('off', 245), ('-PRON-', 246), ('go', 247), ('and', 248), ('start', 249), ('to', 250), ('run', 251), ('for', 252), ('takeoff', 253), ('the', 254), ('first', 255), ('and', 256), ('the', 257), ('second', 258), ('run', 259), ('weren', 260), ('t', 261), ('fast', 262), ('enough', 263), ('but', 264), ('at', 265), ('the', 266), ('third', 267), ('try', 268), ('-PRON-', 269), ('finally', 270), ('get', 271), ('off', 272), ('the', 273), ('ground', 274), ('flap', 275), ('-PRON-', 276), ('wing', 277), ('and', 278), ('wave', 279), ('-PRON-', 280), ('tail', 281), ('not', 282), ('so', 283), ('fast', 284), ('shout', 285), ('somebody', 286), ('and', 287), ('another', 288), ('voice', 289), ('yell', 290), ('faster', 291), ('or', 292), ('-PRON-', 293), ('will', 294), ('end', 295), ('up', 296), ('in', 297), ('the', 298), ('tree', 299), ('the', 300), ('dragon', 301), ('reply', 302), ('oh', 303), ('bother', 304), ('i', 305), ('m', 306), ('do', 307), ('the', 308), ('good', 309), ('i', 310), ('can', 311), ('why', 312), ('don', 313), ('t', 314), ('-PRON-', 315), ('lot', 316), ('keep', 317), ('still', 318), ('for', 319), ('once', 320), ('the', 321), ('fact', 322), ('be', 323), ('that', 324), ('because', 325), ('-PRON-', 326), ('be', 327), ('frightened', 328), ('-PRON-', 329), ('do', 330), ('everything', 331), ('but', 332), ('keep', 333), ('still', 334), ('and', 335), ('so', 336), ('after', 337), ('a', 338), ('while', 339), ('the', 340), ('poor', 341), ('red', 342), ('dragon', 343), ('now', 344), ('very', 345), ('tired', 346), ('simply', 347), ('could', 348), ('not', 349), ('flap', 350), ('-PRON-', 351), ('wing', 352), ('any', 353), ('longer', 354), ('and', 355), ('crash', 356), ('on', 357), ('a', 358), ('lovely', 359), ('green', 360), ('meadow', 361), ('all', 362), ('the', 363), ('animal', 364), ('shriek', 365), ('with', 366), ('terror', 367), ('nobody', 368), ('lose', 369), ('-PRON-', 370), ('life', 371), ('but', 372), ('the', 373), ('snake', 374), ('lose', 375), ('-PRON-', 376), ('leg', 377), ('and', 378), ('slither', 379), ('away', 380), ('through', 381), ('the', 382), ('grass', 383), ('the', 384), ('rhino', 385), ('bump', 386), ('-PRON-', 387), ('head', 388), ('and', 389), ('grow', 390), ('a', 391), ('horn', 392), ('all', 393), ('the', 394), ('elephant', 395), ('s', 396), ('tooth', 397), ('fall', 398), ('out', 399), ('except', 400), ('for', 401), ('two', 402), ('which', 403), ('become', 404), ('very', 405), ('long', 406), ('the', 407), ('giraffe', 408), ('sprain', 409), ('-PRON-', 410), ('neck', 411), ('and', 412), ('-PRON-', 413), ('grow', 414), ('to', 415), ('a', 416), ('great', 417), ('length', 418), ('the', 419), ('hippo', 420), ('roll', 421), ('about', 422), ('so', 423), ('much', 424), ('-PRON-', 425), ('become', 426), ('nearly', 427), ('round', 428), ('end', 429), ('up', 430), ('in', 431), ('a', 432), ('pond', 433), ('and', 434), ('didn', 435), ('t', 436), ('come', 437), ('out', 438), ('-PRON-', 439), ('be', 440), ('too', 441), ('ashamed', 442), ('to', 443), ('be', 444), ('see', 445), ('well', 446), ('in', 447), ('that', 448), ('fall', 449), ('all', 450), ('the', 451), ('animal', 452), ('take', 453), ('on', 454), ('a', 455), ('different', 456), ('appearance', 457), ('and', 458), ('become', 459), ('what', 460), ('-PRON-', 461), ('be', 462), ('today', 463), ('and', 464), ('when', 465), ('the', 466), ('lion', 467), ('see', 468), ('-PRON-', 469), ('what', 470), ('-PRON-', 471), ('say', 472), ('be', 473), ('oh', 474), ('how', 475), ('funny', 476), ('-PRON-', 477), ('look', 478)]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ySq0jJn8oexi"
      },
      "source": [
        "with open('DocTerms_SpacyLemm.pickle', 'wb') as handle:\n",
        "  pickle.dump(corpus, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "with open('/content/DocTerms_SpacyLemm.pickle', 'rb') as handle:\n",
        "  docT = pickle.load(handle)"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vzB7BtfVKu49",
        "outputId": "a626c698-dbed-4159-ce6e-7f1c54e3576f"
      },
      "source": [
        "docT.keys()"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['tree.txt', 'consumdr.hum', 'snow.txt', 'contrad1.hum', 'candle.hum', 'spiders.txt', 'timem.hac', 'aluminum.hum', 'life.txt', 'sight.txt', 'cameloto.hum', 'beyond.hum', 'partya.txt', 'nitepeek.sto', 'blind.txt', 'altside.hum', 'abyss.txt', 't_zone.jok', 'fantas.hum', 'advsayed.txt', 'eyeargon.hum', 'elite.app', 'rocket.sf', 'corcor.hum', 'elveshoe.txt', 'ab40thv.txt', 'gold3ber.txt', 'game.txt', 'excerpt.txt', 'knuckle.txt', 'emperor3.txt', 'empnclot.txt', 'abbey.txt', 'advtthum.txt', 'archive', 'wolfcran.txt', 'immorti.hum', 'wlgirl.txt', 'cooldark.sto', 'wolf7kid.txt', 'vday.hum', 'imagin.hum', 'adv_alad.txt', 'wombat.und', 'gemdra.txt', 'aircon.txt', 'wolflamb.txt', 'testpilo.hum', 'valen', 'confilct.fun', 'narciss.txt', 'enchdup.hum', 'ladylust.hum', 'tcoa.txt', 'enginer.txt', 'encamp01.txt', 'korea.s', 'taxnovel.txt', 'greedog.txt', 'keepmodu.txt', 'vainsong.txt', 'goldgoos.txt', 'quickfix', 'goldbug.poe', 'omarsheh.txt', 'grav', 'empty.txt', 'oxfrog.txt', 'gulliver.txt', 'obstgoat.txt', 'gay', 'ghost', 'goldfish.txt', 'tao3.dos', 'empsjowk.txt', 'enya_trn.txt', 'quot', 'kharian.txt', 'ezoff', 'girlclub.txt', 'tctac.txt', 'gatherng.txt', 'tailbear.txt', 'traitor.txt', 'gloves.txt', 'vampword.txt', 'girl', 'vaincrow.txt', 'enc', 'qcarroll', 'goldenp.txt', 'greatlrn.leg', 'yukon.txt', 'veiledl.txt', 'write', 'unluckwr.txt', 'wrt', 'uglyduck.txt', 'weeprncs.txt', 'lgoldbrd.txt', 'aminegg.txt', 'alissadl.txt', 'antcrick.txt', 'aquith.txt', 'beast.asc', 'bulzork1.txt', 'bulironb.txt', 'bureau.txt', 'beautbst.txt', 'bulphrek.txt', 'bgcspoof.txt', 'bulfelis.txt', 'burintrv.66', 'burltrs', 'burintrv.92', 'burintrv.78', 'rid.txt', 'mindprob.txt', 'lament.txt', 'kzap.txt', 'sucker.txt', 'mike.txt', 'angry_ca.txt', 'asop', 'mario.txt', 'zombies.txt', 'wisteria.txt', 'outcast.dos', 'quest', 'withdraw.cyb', 'beggars.txt', 'buggy.txt', 'blue', 'bishop00.txt', 'bern', 'blackp.txt', 'buldetal.txt', 'bulhuntr.txt', 'blh.txt', 'buldream.txt', 'bulmrx.txt', 'blackrdr', 'blak', 'bulprint.txt', 'bulolli1.txt', 'bulnoopt.txt', 'bulnland.txt', 'bulolli2.txt', 'graymare.txt', 'batlslau.txt', 'blossom.pom', 'bluebrd.txt', 'bullove.txt', 'bumm', 'burn', 's&m_plot', 's&m_that', 'safe', 'arcadia.sty', 'weaver.txt', 'tuc_mees', 'sanpedr2.txt', 'mattress.txt', 'mazarin.txt', 'melissa.txt', 'tearglas.txt', 'thanksg', 'thewave', 'the-tree.txt', 'timetrav.txt', 'tin', 'tinsoldr.txt', 'toilet.s', '13chil.txt', '14.lws', '16.lws', '17.lws', '18.lws', '19.lws', '20.lws', '5orange.txt', '6ablemen.txt', '6napolen.txt', '7oldsamr.txt', '7voysinb.txt', 'musgrave.txt', 'musibrem.txt', 'jackbstl.txt', 'jaynejob.asc', 'jim.asc', '3gables.txt', '3lpigs.txt', '3student.txt', '3wishes.txt', 'radar_ra.txt', 'rainda.txt', 'reap', 'shoscomb.txt', 'shrdfarm.txt', 'shulk.txt', 'sick-kid.txt', 'silverb.txt', 'sis', 'sleprncs.txt', 'snowmaid.txt', 'snowqn1.txt', 'piracy.sto', 'panama.txt', 'paul_har.sto', 'peace.fun', 'wanderer.fun', 'hansgrtl.txt', 'hareleph.txt', 'hareporc.txt', 'haretort.txt', 'stainles.ana', 'angelfur.hum', 'bigred.hum', 'blabnove.hum', 'blabnove.txt', 'brain.damage', 'bulwer.lytton', 'crazy.hum', 'excerpt.hum', 'fantasy.hum', 'fantasy.txt', 'fred.txt', 'hitch2.txt', 'hitch3.txt', 'hotline1.txt', 'hotline3.txt', 'hotline4.txt', 'idi.hum', 'imonly17.txt', 'jerichms.hum', 'fear.hum', 'dakota.txt', 'dan', 'darkness.txt', 'deal', 'bram', 'bran', 'breaks1.asc', 'breaks2.asc', 'breaks3.asc', 'bruce-p.txt', 'lil', 'lionbird', 'lionmane.txt', 'lionmosq.txt', 'lionwar.txt', 'lmermaid.txt', 'lmtchgrl.txt', 'startrek.txt', 'deathmrs.d', 'deer.txt', 'descent.poe', 'diaryflf.txt', 'dicegame.txt', 'dicksong.txt', 'myeyes', 'long1-3.txt', 'lpeargrl.txt', 'lrrhood.txt', 'ltp', 'luf', 'lure.txt', 'fleas.txt', 'flktrp.txt', 'floc', 'floobs.txt', 'flute.txt', 'flytrunk.txt', 'paink-ws.txt', 'parotsha.txt', 'perf', 'mtinder.txt', 'monkking.txt', 'monksol.txt', 'mouslion.txt', 'mindwar', 'missing.txt', 'pussboot.txt', 'pinocch.txt', 'foxncrow.txt', 'foxnstrk.txt', 'mydream.txt', 'cabin.txt', 'cardcnt.txt', 'ccm.txt', 'domain.poe', 'dopedenn.txt', 'dskool.txt', 'dtruck.txt', 'dwar', 'redragon.txt', 'retrib.txt', 'rock', 'roger1.txt', 'running.txt', 'sunday.txt', 'superg1', 'stairdre.txt', 'stsgreek', 'igiv', 'immortal', 'inter', 'adler.txt', 'aesop11.txt', 'aesopa10.txt', 'alad10.txt', 'healer.txt', 'whgdsreg.reg', 'fable.txt', 'space.txt', 'spectacl.poe', 'spider.txt', 'sqzply.txt', 'sre-dark.txt', 'szechuan', 'solitary.txt', 'pregn.txt', 'psf.txt', 'psi', 'psyc', 'plescopm.txt', 'cybersla.txt', 'hole2nar.txt', 'holmesbk.txt', 'home.fil', 'hop-frog.poe', 'horsdonk.txt', 'horswolf.txt', 'hound-b.txt', 'fic1', 'fic2', 'fic3', 'fic4', 'fic5', 'fic7', 'fish.txt', 'frogp.txt', 'island.poe', 'foxngrap.txt', 'fran', 'fea1', 'fea2', 'fea3', 'fearmnky', 'fgoose.txt', 'freeman.fil', 'friend.s', 'friends.txt', 'frum', 'nigel.10', 'nigel.2', 'nigel.3', 'nigel.4', 'nigel.5', 'nigel.6', 'nigel.7', 'nihgel_8.9', 'non2', 'non3', 'non4', 'nigel.1', '4moons.txt', 'telefone.txt', 'hils', 'history5.txt', 'poplstrm.txt', 'pphamlin.txt', 'prince.art', 'progx', 'hell4.txt', 'helmfuse.txt', 'charlie.txt', 'chik', 'clevdonk.txt', 'clon', 'cmoutmou.txt', 'comp', 'crabhern.txt', 'cum', 'wall.art', 'blasters.fic', 'jackmac.fic', 'reality.txt', 'times.fic', 'fourth.fic', 'campfire.txt', 'aislesix.txt', 'bagelman.txt', 'berternie.txt', 'discocanbefun.txt', 'kneeslapper.txt', 'mcdonaldl.txt', 'modemhippy.txt', 'pepdegener.txt', 'socialvikings.txt', 'terrorbears.txt', 'bgb.txt', 'cooldark.txt', 'aisle.six', 'bagel.man', 'cow.exploder', 'curious.george', 'day.in.mcdonald', 'disco.be.fun', 'fowl.death', 'how.ernie.bert', 'keeping.insanit', 'kneeslapper', 'pepsi.degenerat', 'social.vikings', 'spam.key', 'textfile.primer', 'robotech', 'hellmach.txt', '3sonnets.vrs', 'glimpse1.txt', 'bookem2', 'bookem.1', 'bookem3', '100west.txt', 'assorted.txt', 'arctic.txt', 'bestwish', 'forgotte', 'quarter.c1', 'quarter.c10', 'quarter.c11', 'quarter.c12', 'quarter.c13', 'quarter.c14', 'quarter.c15', 'quarter.c16', 'quarter.c17', 'quarter.c18', 'quarter.c19', 'quarter.c2', 'quarter.c3', 'quarter.c4', 'quarter.c5', 'quarter.c6', 'quarter.c7', 'quarter.c8', 'quarter.c9', 'vgilante.txt', 'sre02.txt', 'sre03.txt', 'sretrade.txt', 'sre01.txt', 'sre_feqh.txt', 'sre_sei.txt', 'sre05.txt', 'sre07.txt', 'sre10.txt', 'sre09.txt', 'sre06.txt', 'sre_finl.txt', 'sre08.txt', 'srex.txt', 'sre04.txt', 'poem-1.txt', 'poem-2.txt', 'poem-4.txt'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OvzYl30rkd4O"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}