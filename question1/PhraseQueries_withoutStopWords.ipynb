{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PhraseQueries_withoutStopWords.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iTLJfR_zIpIG",
        "outputId": "ebc34937-6d90-45f6-8846-2ff6ee5402fd"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ikOL7PmhwIcG",
        "outputId": "21f6fe35-ea6d-448e-c64d-55b1fec19257"
      },
      "source": [
        "import joblib\n",
        "import functools\n",
        "import copy\n",
        "import operator\n",
        "import pickle\n",
        "import spacy\n",
        "import os\n",
        "import re, string, unicodedata\n",
        "import nltk\n",
        "!pip install contractions\n",
        "!pip install inflect\n",
        "import contractions\n",
        "import inflect\n",
        "from bs4 import BeautifulSoup\n",
        "from nltk import word_tokenize, sent_tokenize\n",
        "from nltk.corpus import stopwords,wordnet\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting contractions\n",
            "  Downloading https://files.pythonhosted.org/packages/0a/04/d5e0bb9f2cef5d15616ebf68087a725c5dbdd71bd422bcfb35d709f98ce7/contractions-0.0.48-py2.py3-none-any.whl\n",
            "Collecting textsearch>=0.0.21\n",
            "  Downloading https://files.pythonhosted.org/packages/d3/fe/021d7d76961b5ceb9f8d022c4138461d83beff36c3938dc424586085e559/textsearch-0.0.21-py2.py3-none-any.whl\n",
            "Collecting pyahocorasick\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7f/c2/eae730037ae1cbbfaa229d27030d1d5e34a1e41114b21447d1202ae9c220/pyahocorasick-1.4.2.tar.gz (321kB)\n",
            "\u001b[K     |████████████████████████████████| 327kB 4.9MB/s \n",
            "\u001b[?25hCollecting anyascii\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/09/c7/61370d9e3c349478e89a5554c1e5d9658e1e3116cc4f2528f568909ebdf1/anyascii-0.1.7-py3-none-any.whl (260kB)\n",
            "\u001b[K     |████████████████████████████████| 266kB 7.7MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyahocorasick\n",
            "  Building wheel for pyahocorasick (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyahocorasick: filename=pyahocorasick-1.4.2-cp37-cp37m-linux_x86_64.whl size=85398 sha256=4fa0beb70d22e6b5aaae3840408ead15b6a8199dd43042f5acbeaf9693df7b6c\n",
            "  Stored in directory: /root/.cache/pip/wheels/3a/03/34/77e3ece0bba8b86bfac88a79f923b36d805cad63caeba38842\n",
            "Successfully built pyahocorasick\n",
            "Installing collected packages: pyahocorasick, anyascii, textsearch, contractions\n",
            "Successfully installed anyascii-0.1.7 contractions-0.0.48 pyahocorasick-1.4.2 textsearch-0.0.21\n",
            "Requirement already satisfied: inflect in /usr/local/lib/python3.7/dist-packages (2.1.0)\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B1NJk_R2FChT"
      },
      "source": [
        "def strip_html(text):\n",
        "    soup = BeautifulSoup(text, \"html.parser\")\n",
        "    return soup.get_text()\n",
        "\n",
        "def remove_between_square_brackets(text):\n",
        "    text=re.sub('\\n',' ',text)\n",
        "    return re.sub('\\[[^]]*\\]', '', text)\n",
        "\n",
        "def denoise_text(text):\n",
        "    text = strip_html(text)\n",
        "    text = remove_between_square_brackets(text)\n",
        "    return text\n",
        "\n",
        "def replace_contractions(text):\n",
        "    return contractions.fix(text)\n",
        "\n",
        "def remove_non_ascii(words):\n",
        "    \"\"\"Remove non-ASCII characters from list of tokenized words\"\"\"\n",
        "    new_words = []\n",
        "    for word in words:\n",
        "        new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
        "        new_words.append(new_word)\n",
        "    return new_words\n",
        "\n",
        "def to_lowercase(words):\n",
        "    \"\"\"Convert all characters to lowercase from list of tokenized words\"\"\"\n",
        "    new_words = []\n",
        "    for word in words:\n",
        "        new_word = word.lower()\n",
        "        new_words.append(new_word)\n",
        "    return new_words\n",
        "\n",
        "def remove_punctuation(words):\n",
        "    \"\"\"Remove punctuation from list of tokenized words\"\"\"\n",
        "    new_words = []\n",
        "    for word in words:\n",
        "        new_word = re.sub(r'[^\\w\\s]', '', word)\n",
        "        if new_word != '':\n",
        "            new_words.append(new_word)\n",
        "    return new_words\n",
        "\n",
        "def remove_stopwords(words):\n",
        "    \"\"\"Remove stop words from list of tokenized words\"\"\"\n",
        "    new_words = []\n",
        "    for word in words:\n",
        "        if word not in stopwords.words('english'):\n",
        "            new_words.append(word)\n",
        "    return new_words\n",
        "\n",
        "def get_wordnet_pos(tag):\n",
        "    tag_dict = {\"J\": wordnet.ADJ,\"N\": wordnet.NOUN,\"V\": wordnet.VERB,\"R\": wordnet.ADV}\n",
        "    return tag_dict.get(tag,wordnet.NOUN)\n",
        "\n",
        "# def lemmatize(words):\n",
        "#     \"\"\"Lemmatize words in list of tokenized words\"\"\"\n",
        "#     lemmatizer = WordNetLemmatizer()\n",
        "#     lemmas = []\n",
        "#     posTagged = nltk.pos_tag(words)\n",
        "#     wordnetTagged = list(map(lambda x: (x[0], get_wordnet_pos(x[1][0])), posTagged))\n",
        "#     for word,tag in wordnetTagged:\n",
        "#       lemma = lemmatizer.lemmatize(word,tag)\n",
        "#       lemmas.append(lemma)\n",
        "#     return lemmas\n",
        "\n",
        "def lemmatizeSpacy(words):\n",
        "  sent = \"\"\n",
        "  lwords = []\n",
        "  for word in words:\n",
        "    sent += word + \" \" \n",
        "  model = spacy.load(\"en_core_web_sm\")\n",
        "  tokens = model(sent)\n",
        "  # for i in range(len(tokens)):\n",
        "  #   lwords.append((tokens[i].lemma_,i+1))\n",
        "  for token in tokens:\n",
        "    lwords.append(token.lemma_)\n",
        "  return lwords\n",
        "\n",
        "def preProcess_html(fileName):\n",
        "    sample = denoise_text(fileName)\n",
        "    sample = replace_contractions(sample)\n",
        "    words = nltk.word_tokenize(sample)\n",
        "    words = remove_non_ascii(words)\n",
        "    words = to_lowercase(words)\n",
        "    words = remove_punctuation(words)\n",
        "    words = lemmatizeSpacy(words)\n",
        "    words = remove_stopwords(words)\n",
        "    # words = lemmatize(words)\n",
        "    # words = lemmatizeSpacy(words)\n",
        "    return words  \n",
        "\n",
        "def clean_text(text):\n",
        "    # text=re.sub('\\w*\\d\\w*','', text)\n",
        "    text=re.sub('\\n',' ',text)\n",
        "    text=re.sub(r\"http\\S+\", \"\", text)\n",
        "    text=re.sub('[^a-z0-9A-Z]',' ',text)\n",
        "    text=re.sub(' +',' ',text)\n",
        "    return text\n",
        "\n",
        "def preProcessSentence(fileName):\n",
        "    sample = clean_text(fileName)\n",
        "    sample = replace_contractions(sample)\n",
        "    words = nltk.word_tokenize(sample)\n",
        "    words = remove_non_ascii(words)\n",
        "    words = to_lowercase(words)\n",
        "    words = remove_punctuation(words)\n",
        "    words = remove_stopwords(words)\n",
        "    words = lemmatizeSpacy(words)    \n",
        "    return words"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R2fX6B6Vyjbi"
      },
      "source": [
        "def getDocNames(dt,ListdocId):\n",
        "  Docnames = []\n",
        "  for Sdocid in ListdocId:\n",
        "    for docname, docid in dt.items():  \n",
        "      if Sdocid == docid:\n",
        "        Docnames.append(docname)\n",
        "  return Docnames\n",
        "\n",
        "def oneWord(Q,positional_index,document_index_map):\n",
        "  if Q not in positional_index.keys():\n",
        "    print(\"Terms Not Present\")\n",
        "    print(\"Number of documents retrieved: {0}\".format(0))\n",
        "    print(\"List of document names retrieved: --\")\n",
        "  else:\n",
        "    p = positional_index[Q][1].keys()\n",
        "    Listdocs = getDocNames(document_index_map,p)\n",
        "    print(\"Number of documents retrieved: {0}\".format(len(Listdocs)))\n",
        "    print(\"List of document names retrieved: {0}\".format(\", \".join(Listdocs)))\n",
        "\n",
        "def getPostings(terms,dt):\n",
        "    #all terms in the list are guaranteed to be in the index\n",
        "    return [ dt[term] for term in terms ]\n",
        "\n",
        "\n",
        "def getDocsFromPostings(postings):\n",
        "    #no empty list in postings\n",
        "    return [ [x[0] for x in p] for p in postings ]\n",
        "\n",
        "def intersectLists(lists):\n",
        "    if len(lists)==0:\n",
        "        return []\n",
        "    lists.sort(key=len)\n",
        "    return list(functools.reduce(lambda x,y: set(x)&set(y),lists))\n",
        "\n",
        "def getDictionary(query,positional_index):\n",
        "  d = {}\n",
        "  for i in query:\n",
        "    d[i] = [[x,positional_index[i][1][x]] for x in positional_index[i][1]]\n",
        "  return d\n",
        "\n",
        "def phraseQueries(Query,positional_index,document_index_map):\n",
        "  for term in Query:\n",
        "    if term not in positional_index.keys():\n",
        "      print(\"Terms Not Found\")\n",
        "      print(\"Number of documents retrieved: {0}\".format(0))\n",
        "      print(\"List of document names retrieved: --\")\n",
        "      return\n",
        "  dictionary = getDictionary(Query,positional_index)\n",
        "  postings = getPostings(Query,dictionary)\n",
        "  # print(postings)\n",
        "  docs = getDocsFromPostings(postings)\n",
        "  # print(docs)\n",
        "  docs = intersectLists(docs) \n",
        "  # print(docs)\n",
        "  for i in range(len(postings)):\n",
        "    postings[i]=[x for x in postings[i] if x[0] in docs]\n",
        "  # print(postings)\n",
        "  postings=copy.deepcopy(postings)\n",
        "  for i in range(len(postings)):\n",
        "    for j in range(len(postings[i])):\n",
        "      postings[i][j][1]=[x-i for x in postings[i][j][1]]\n",
        "\n",
        "  # print(postings)\n",
        "  result=[]\n",
        "  for i in range(len(postings[0])):\n",
        "    li=intersectLists([x[i][1] for x in postings])\n",
        "    if li==[]:\n",
        "      continue\n",
        "    else:\n",
        "      result.append(postings[0][i][0])    #append the docid to the result\n",
        "  if len(result) == 0:\n",
        "    print(\"Number of documents retrieved: {0}\".format(0))\n",
        "    print(\"List of document names retrieved: --\")\n",
        "  else:\n",
        "    print(\"Number of documents retrieved: {0}\".format(len(result)))\n",
        "    print(\"List of document names retrieved: {0}\".format(\", \".join(getDocNames(document_index_map,result))))"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x-NxxNOfQXxg",
        "outputId": "09d0578a-4104-4511-f190-0e0b35ee9ea0"
      },
      "source": [
        "if __name__== \"__main__\":\n",
        "  positional_index = joblib.load('/content/drive/MyDrive/IR_ASSIGNMENT_1/without_StopWords/positional_indexes')\n",
        "  document_index_map = joblib.load('/content/drive/MyDrive/IR_ASSIGNMENT_1/without_StopWords/document_index_map')\n",
        "  document_index_map_keys = list(document_index_map.keys())\n",
        "  document_index_map_values = list(document_index_map.values())\n",
        "  number_of_queries = int(input(\"Enter number of queries:   \"))\n",
        "  # Queries = [\"plum-coloured swelling\",\"Telephones jangle and typewriters\",\"good government\",\"the mountains bordering\",\"good and day\",\"good day\",\"rabbits, hares, partridges and skylarks\",\"The terrified citizens flocked\",\"Sherlock Holmes\",\"the lion, the tiger\",\"lion tiger\",\"I Cried\",\"British bark Sophy Anderson\",\"Gfile Distribution Center\",\"Abbey Grange, Marsham\",\"Descent and Easy\",\"AND ON THAT BRANCH THERE WAS A NEST\"]\n",
        "  for q in range(number_of_queries):\n",
        "    query = str(input(\"Enter the query with terms separated by space:  \"))\n",
        "    print(\"Query : \"+query)\n",
        "    if(not (query and not query.isspace())):\n",
        "      print(\"Empty Query\")\n",
        "    else:\n",
        "      query = preProcessSentence(query)\n",
        "      print(\"Query Terms: {0}\".format(query))\n",
        "      if len(query) == 1:\n",
        "        oneWord(query[0],positional_index,document_index_map)\n",
        "      else:\n",
        "        phraseQueries(query,positional_index,document_index_map)\n",
        "    \n",
        "    print(\"\\n\")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Enter number of queries:   2\n",
            "Enter the query with terms separated by space:  good day\n",
            "Query : good day\n",
            "Query Terms: ['good', 'day']\n",
            "Number of documents retrieved: 22\n",
            "List of document names retrieved: enchdup.hum, gulliver.txt, outcast.dos, mazarin.txt, melissa.txt, 13chil.txt, sick-kid.txt, brain.damage, fantasy.hum, fantasy.txt, breaks2.asc, bruce-p.txt, startrek.txt, superg1, aesop11.txt, aesopa10.txt, horswolf.txt, hound-b.txt, fic5, history5.txt, forgotte, srex.txt\n",
            "\n",
            "\n",
            "Enter the query with terms separated by space:  good and day\n",
            "Query : good and day\n",
            "Query Terms: ['good', 'day']\n",
            "Number of documents retrieved: 22\n",
            "List of document names retrieved: enchdup.hum, gulliver.txt, outcast.dos, mazarin.txt, melissa.txt, 13chil.txt, sick-kid.txt, brain.damage, fantasy.hum, fantasy.txt, breaks2.asc, bruce-p.txt, startrek.txt, superg1, aesop11.txt, aesopa10.txt, horswolf.txt, hound-b.txt, fic5, history5.txt, forgotte, srex.txt\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qbt7ItN3QXtQ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6YYWu3OHDIyV"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}